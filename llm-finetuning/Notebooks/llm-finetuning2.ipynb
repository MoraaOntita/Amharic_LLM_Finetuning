{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyamusi_ontita/miniconda3/envs/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM, \n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import unicodedata\n",
    "import string\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "from nltk import BigramCollocationFinder\n",
    "import nltk.collocations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyamusi_ontita/miniconda3/envs/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"iocuydi/llama-2-amharic-3784m\"\n",
    "commit_hash = \"04fcac974701f1dab0b8e39af9d3ecfce07b3773\"\n",
    "# The commit hash is needed, because the model repo was rearranged after this commit (files -> finetuned/files),\n",
    "# and I couldn't load the model from the new structure\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(checkpoint, revision =commit_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model and tokenizer loading\n",
    "model_name = \"NousResearch/Llama-2-7b-hf\"\n",
    "llama_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(51008, 4096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.resize_token_embeddings(len(tokenizer)) # needed because the fine-tuned model extended the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we want:\n",
    "model = PeftModel.from_pretrained(llama_model, \"iocuydi/llama-2-amharic-3784m\",revision =commit_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded simonbutt/amharic_truthful_qa successfully.\n",
      "Loaded simonbutt/amharic_gsm8k successfully.\n",
      "Loaded EthioNLP/Amharic_LLAMA_MT successfully.\n",
      "Loaded EthioNLP/Amharic_Instruction_dataset successfully.\n",
      "Loaded Tvsybkzkmapab/Amharic_ad_generation successfully.\n",
      "Loaded BiniyamAjaw/amharic_dataset_v2 successfully.\n",
      "Loaded Henok/amharic-qa successfully.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# List of dataset names\n",
    "dataset_names = [\n",
    "    \"simonbutt/amharic_truthful_qa\",\n",
    "    \"simonbutt/amharic_gsm8k\",\n",
    "    \"EthioNLP/Amharic_LLAMA_MT\",\n",
    "    \"EthioNLP/Amharic_Instruction_dataset\",\n",
    "    \"Tvsybkzkmapab/Amharic_ad_generation\",\n",
    "    \"BiniyamAjaw/amharic_dataset_v2\",\n",
    "    \"Henok/amharic-qa\"\n",
    "]\n",
    "\n",
    "# Dictionary to store loaded datasets\n",
    "loaded_datasets = {}\n",
    "\n",
    "# Loop through dataset names and load datasets\n",
    "for dataset_name in dataset_names:\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        loaded_datasets[dataset_name] = dataset\n",
    "        print(f\"Loaded {dataset_name} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {dataset_name}: {e}\")\n",
    "\n",
    "# Example of accessing a specific split of a dataset\n",
    "if \"rasyosef/amharic-news-category-classification\" in loaded_datasets:\n",
    "    dataset = loaded_datasets[\"rasyosef/amharic-news-category-classification\"]\n",
    "    if \"train\" in dataset:\n",
    "        train_data = dataset[\"train\"]\n",
    "        print(f\"Train data size: {len(train_data)}\")\n",
    "    if \"test\" in dataset:\n",
    "        test_data = dataset[\"test\"]\n",
    "        print(f\"Test data size: {len(test_data)}\")\n",
    "    if \"validation\" in dataset:\n",
    "        val_data = dataset[\"validation\"]\n",
    "        print(f\"Validation data size: {len(val_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "class AmharicPreprocessor:\n",
    "    def __init__(self, expansion_file_dir='/home/nyamusi_ontita/Amharic_LLM_Finetuning-1/llm-finetuning/short_forms.txt', bigram_dir='bigrams.txt'):\n",
    "        self.expansion_file_dir = expansion_file_dir\n",
    "        self.bigram_dir = bigram_dir\n",
    "        self.short_form_dict = self.get_short_forms()\n",
    "\n",
    "    def get_short_forms(self):\n",
    "        exp = {}\n",
    "        try:\n",
    "            with open(self.expansion_file_dir, encoding='utf8') as text:\n",
    "                for line in text:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    expanded = line.split(\"-\")\n",
    "                    exp[expanded[0].strip()] = expanded[1].replace(\" \", '_').strip()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {self.expansion_file_dir}\")\n",
    "        return exp\n",
    "\n",
    "    def expand_short_form(self, input_short_word):\n",
    "        return self.short_form_dict.get(input_short_word, input_short_word)\n",
    "\n",
    "    def normalize_char_level_mismatch(self, input_token):\n",
    "        replacements = [\n",
    "            ('[ሃኅኃሐሓኻ]', 'ሀ'), ('[ሑኁዅ]', 'ሁ'), ('[ኂሒኺ]', 'ሂ'), ('[ኌሔዄ]', 'ሄ'), ('[ሕኅ]', 'ህ'),\n",
    "            ('[ኆሖኾ]', 'ሆ'), ('[ሠ]', 'ሰ'), ('[ሡ]', 'ሱ'), ('[ሢ]', 'ሲ'), ('[ሣ]', 'ሳ'), ('[ሤ]', 'ሴ'),\n",
    "            ('[ሥ]', 'ስ'), ('[ሦ]', 'ሶ'), ('[ዓኣዐ]', 'አ'), ('[ዑ]', 'ኡ'), ('[ዒ]', 'ኢ'), ('[ዔ]', 'ኤ'),\n",
    "            ('[ዕ]', 'እ'), ('[ዖ]', 'ኦ'), ('[ጸ]', 'ፀ'), ('[ጹ]', 'ፁ'), ('[ጺ]', 'ፂ'), ('[ጻ]', 'ፃ'),\n",
    "            ('[ጼ]', 'ፄ'), ('[ጽ]', 'ፅ'), ('[ጾ]', 'ፆ'), ('(ሉ[ዋአ])', 'ሏ'), ('(ሙ[ዋአ])', 'ሟ'),\n",
    "            ('(ቱ[ዋአ])', 'ቷ'), ('(ሩ[ዋአ])', 'ሯ'), ('(ሱ[ዋአ])', 'ሷ'), ('(ሹ[ዋአ])', 'ሿ'),\n",
    "            ('(ቁ[ዋአ])', 'ቋ'), ('(ቡ[ዋአ])', 'ቧ'), ('(ቹ[ዋአ])', 'ቿ'), ('(ሁ[ዋአ])', 'ኋ'),\n",
    "            ('(ኑ[ዋአ])', 'ኗ'), ('(ኙ[ዋአ])', 'ኟ'), ('(ኩ[ዋአ])', 'ኳ'), ('(ዙ[ዋአ])', 'ዟ'),\n",
    "            ('(ጉ[ዋአ])', 'ጓ'), ('(ደ[ዋአ])', 'ዷ'), ('(ጡ[ዋአ])', 'ጧ'), ('(ጩ[ዋአ])', 'ጯ'),\n",
    "            ('(ጹ[ዋአ])', 'ጿ'), ('(ፉ[ዋአ])', 'ፏ'), ('[ቊ]', 'ቁ'), ('[ኵ]', 'ኩ')\n",
    "        ]\n",
    "        for pattern, replacement in replacements:\n",
    "            input_token = re.sub(pattern, replacement, str(input_token))\n",
    "        return input_token\n",
    "\n",
    "    def remove_punc_and_special_chars(self, text):\n",
    "        return re.sub(r'[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣]', '', text)\n",
    "\n",
    "    def remove_ascii_and_numbers(self, text_input):\n",
    "        rm_num_and_ascii = re.sub('[A-Za-z0-9]', '', text_input)\n",
    "        return re.sub('[\\u1369-\\u137C]+', '', rm_num_and_ascii)\n",
    "\n",
    "\n",
    "    def arabic2geez(self, arabicNumber):\n",
    "        ETHIOPIC_ONE = 0x1369\n",
    "        ETHIOPIC_TEN = 0x1372\n",
    "        ETHIOPIC_HUNDRED = 0x137B\n",
    "        ETHIOPIC_TEN_THOUSAND = 0x137C\n",
    "\n",
    "        arabicNumber = str(arabicNumber)\n",
    "        n = len(arabicNumber) - 1\n",
    "        if n % 2 == 0:\n",
    "            arabicNumber = \"0\" + arabicNumber\n",
    "            n += 1\n",
    "\n",
    "        arabicBigrams = [arabicNumber[i:i+2] for i in range(0, n, 2)]\n",
    "        reversedArabic = arabicBigrams[::-1]\n",
    "        geez = []\n",
    "\n",
    "        for index, pair in enumerate(reversedArabic):\n",
    "            curr_geez = ''\n",
    "            artens = pair[0]\n",
    "            arones = pair[1]\n",
    "            amtens = ''\n",
    "            amones = ''\n",
    "            if artens != '0':\n",
    "                amtens = str(chr((int(artens) + (ETHIOPIC_TEN - 1))))\n",
    "            else:\n",
    "                if arones == '0':\n",
    "                    continue\n",
    "            if arones != '0':\n",
    "                amones = str(chr((int(arones) + (ETHIOPIC_ONE - 1))))\n",
    "            if index > 0:\n",
    "                if index % 2 != 0:\n",
    "                    curr_geez = amtens + amones + str(chr(ETHIOPIC_HUNDRED))\n",
    "                else:\n",
    "                    curr_geez = amtens + amones + str(chr(ETHIOPIC_TEN_THOUSAND))\n",
    "            else:\n",
    "                curr_geez = amtens + amones\n",
    "            geez.append(curr_geez)\n",
    "\n",
    "        geez = ''.join(geez[::-1])\n",
    "        if geez.startswith('፩፻') or geez.startswith('፩፼'):\n",
    "            geez = geez[1:]\n",
    "\n",
    "        if len(arabicNumber) >= 7:\n",
    "            end_zeros = ''.join(re.findall('([0]+)$', arabicNumber)[0:])\n",
    "            i = int(len(end_zeros) / 3)\n",
    "            if len(end_zeros) >= (3 * i):\n",
    "                if i >= 3:\n",
    "                    i -= 1\n",
    "                for thoushand in range(i - 1):\n",
    "                    geez += '፼'\n",
    "\n",
    "        return geez\n",
    "\n",
    "    def get_expanded_number(self, number):\n",
    "        if '.' not in str(number):\n",
    "            return self.arabic2geez(number)\n",
    "        else:\n",
    "            num, decimal = str(number).split('.')\n",
    "            if decimal.startswith('0'):\n",
    "                decimal = decimal[1:]\n",
    "                dot = ' ነጥብ ዜሮ '\n",
    "            else:\n",
    "                dot = ' ነጥብ '\n",
    "            return self.arabic2geez(num) + dot + self.arabic2geez(decimal)\n",
    "\n",
    "    def tokenize(self, corpus):\n",
    "        sentences = re.compile('[!?።\\፡\\፡]+').split(corpus)\n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            tokens.extend(sentence.split())\n",
    "        return tokens\n",
    "\n",
    "    def collocation_finder(self, tokens):\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        finder.apply_freq_filter(3)\n",
    "        frequent_bigrams = finder.nbest(bigram_measures.chi_sq, 5)\n",
    "\n",
    "        with io.open(self.bigram_dir, \"w\", encoding=\"utf8\") as PhraseWriter:\n",
    "            for bigram in frequent_bigrams:\n",
    "                PhraseWriter.write(bigram[0] + ' ' + bigram[1] + \"\\n\")\n",
    "\n",
    "    def normalize_multi_words(self, tokenized_sentence, corpus):\n",
    "        bigram = set()\n",
    "        sent_with_bigrams = []\n",
    "        index = 0\n",
    "\n",
    "        if not os.path.exists(self.bigram_dir):\n",
    "            self.collocation_finder(self.tokenize(corpus))\n",
    "\n",
    "        try:\n",
    "            with open(self.bigram_dir, encoding='utf8') as phrase_file:\n",
    "                for line in phrase_file:\n",
    "                    bigram.add(tuple(line.strip().split()))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {self.bigram_dir}\")\n",
    "\n",
    "        while index < len(tokenized_sentence):\n",
    "            if index + 1 < len(tokenized_sentence) and (tokenized_sentence[index], tokenized_sentence[index + 1]) in bigram:\n",
    "                sent_with_bigrams.append(\n",
    "                    (tokenized_sentence[index] + \"_\" + tokenized_sentence[index + 1]))\n",
    "                index += 1\n",
    "            else:\n",
    "                sent_with_bigrams.append(tokenized_sentence[index])\n",
    "            index += 1\n",
    "        return sent_with_bigrams\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = self.expand_short_form(text)\n",
    "        text = self.normalize_char_level_mismatch(text)\n",
    "        text = self.remove_punc_and_special_chars(text)\n",
    "        text = self.remove_ascii_and_numbers(text)\n",
    "        return text\n",
    "\n",
    "\n",
    "def load_huggingface_dataset(dataset_name):\n",
    "    return load_dataset(dataset_name)\n",
    "\n",
    "def preprocess_dataset(dataset, text_field, preprocessor):\n",
    "    def preprocess_example(example):\n",
    "        if text_field in example:\n",
    "            example[text_field] = preprocessor.preprocess_text(example[text_field])\n",
    "        return example\n",
    "\n",
    "    return dataset.map(preprocess_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your preprocessor\n",
    "preprocessor = AmharicPreprocessor()\n",
    "\n",
    "# Define the field containing text data in your dataset\n",
    "text_field = \"text\"  # Adjust this according to your dataset structure\n",
    "\n",
    "# Preprocess the dataset\n",
    "for dataset_name in loaded_datasets:\n",
    "    dataset = loaded_datasets[dataset_name]\n",
    "    if isinstance(dataset, dict):\n",
    "        # If the loaded dataset is a dictionary containing splits (e.g., train, test, validation)\n",
    "        for split_name in dataset:\n",
    "            dataset[split_name] = preprocess_dataset(dataset[split_name], text_field, preprocessor)\n",
    "    else:\n",
    "        # If the loaded dataset is just a single dataset\n",
    "        loaded_datasets[dataset_name] = preprocess_dataset(dataset, text_field, preprocessor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the train data and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets with 'train' splits:\n",
      "simonbutt/amharic_gsm8k\n",
      "EthioNLP/Amharic_LLAMA_MT\n",
      "EthioNLP/Amharic_Instruction_dataset\n",
      "Tvsybkzkmapab/Amharic_ad_generation\n",
      "BiniyamAjaw/amharic_dataset_v2\n",
      "Henok/amharic-qa\n"
     ]
    }
   ],
   "source": [
    "# Create a new dictionary to store only the \"train\" splits\n",
    "train_datasets = {}\n",
    "\n",
    "# Iterate through loaded_datasets\n",
    "for dataset_name, dataset in loaded_datasets.items():\n",
    "    # Check if the dataset is a dictionary containing splits\n",
    "    if isinstance(dataset, dict):\n",
    "        # Check if the \"train\" split exists in the dataset\n",
    "        if \"train\" in dataset:\n",
    "            # Add the \"train\" split to the train_datasets dictionary\n",
    "            train_datasets[dataset_name] = dataset[\"train\"]\n",
    "            # Remove the \"train\" split from the original dataset\n",
    "            del dataset[\"train\"]\n",
    "\n",
    "# Print the names of datasets with \"train\" splits\n",
    "print(\"Datasets with 'train' splits:\")\n",
    "for dataset_name in train_datasets:\n",
    "    print(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets with 'validation' splits:\n",
      "simonbutt/amharic_truthful_qa\n",
      "EthioNLP/Amharic_LLAMA_MT\n",
      "EthioNLP/Amharic_Instruction_dataset\n",
      "Tvsybkzkmapab/Amharic_ad_generation\n",
      "Henok/amharic-qa\n"
     ]
    }
   ],
   "source": [
    "# Create a new dictionary to store only the \"validation\" splits\n",
    "validation_datasets = {}\n",
    "\n",
    "# Iterate through loaded_datasets\n",
    "for dataset_name, dataset in loaded_datasets.items():\n",
    "    # Check if the dataset is a dictionary containing splits\n",
    "    if isinstance(dataset, dict):\n",
    "        # Check if the \"validation\" split exists in the dataset\n",
    "        if \"validation\" in dataset:\n",
    "            # Add the \"validation\" split to the validation_datasets dictionary\n",
    "            validation_datasets[dataset_name] = dataset[\"validation\"]\n",
    "            # Remove the \"validation\" split from the original dataset\n",
    "            del dataset[\"validation\"]\n",
    "\n",
    "# Print the names of datasets with \"validation\" splits\n",
    "print(\"Datasets with 'validation' splits:\")\n",
    "for dataset_name in validation_datasets:\n",
    "    print(dataset_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.3,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    save_steps=0,\n",
    "    logging_steps=25\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_datasets,  # Pass the preprocessed training dataset here\n",
    "    eval_dataset=validation_datasets,  # Pass the preprocessed validation dataset here\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1551\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1553\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;66;03m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;66;03m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m total_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mworld_size\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/trainer.py:850\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    842\u001b[0m dataloader_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size,\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_collator,\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpin_memory\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_pin_memory,\n\u001b[1;32m    847\u001b[0m }\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterableDataset):\n\u001b[0;32m--> 850\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampler\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_train_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_drop_last\n\u001b[1;32m    852\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/trainer.py:813\u001b[0m, in \u001b[0;36mTrainer._get_train_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    812\u001b[0m     model_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLengthGroupedSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset)\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:571\u001b[0m, in \u001b[0;36mLengthGroupedSampler.__init__\u001b[0;34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lengths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     model_input_name \u001b[38;5;241m=\u001b[39m model_input_name \u001b[38;5;28;01mif\u001b[39;00m model_input_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[38;5;241m0\u001b[39m], BatchEncoding))\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    573\u001b[0m     ):\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m key.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         )\n\u001b[1;32m    578\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(feature[model_input_name]) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
